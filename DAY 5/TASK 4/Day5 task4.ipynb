{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2bd1bf0-37aa-4563-9e58-9a58c7e7e324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\daksh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ TF-IDF from Scratch (First 5 rows):\n",
      "      nehra   mandeep      four     first  boundary       rcb      full  \\\n",
      "0  0.283486  0.697085  0.038342  0.151717  0.125469  0.189348  0.099640   \n",
      "1  0.384730  0.315348  0.026018  0.102951  0.000000  0.000000  0.000000   \n",
      "2  0.000000  0.000000  0.031674  0.000000  0.000000  0.000000  0.000000   \n",
      "3  0.489657  0.000000  0.033114  0.000000  0.000000  0.000000  0.086052   \n",
      "4  0.347498  0.000000  0.047000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "       pads   needed       put  ...  uhoh  latent  wrath  microsix  whodve  \\\n",
      "0  0.176715  0.20101  0.203775  ...   0.0     0.0    0.0       0.0     0.0   \n",
      "1  0.000000  0.00000  0.000000  ...   0.0     0.0    0.0       0.0     0.0   \n",
      "2  0.000000  0.00000  0.000000  ...   0.0     0.0    0.0       0.0     0.0   \n",
      "3  0.000000  0.00000  0.000000  ...   0.0     0.0    0.0       0.0     0.0   \n",
      "4  0.000000  0.00000  0.000000  ...   0.0     0.0    0.0       0.0     0.0   \n",
      "\n",
      "   outunorthodox  paddlepulls  expresspacer  hastily  runnign  \n",
      "0            0.0          0.0           0.0      0.0      0.0  \n",
      "1            0.0          0.0           0.0      0.0      0.0  \n",
      "2            0.0          0.0           0.0      0.0      0.0  \n",
      "3            0.0          0.0           0.0      0.0      0.0  \n",
      "4            0.0          0.0           0.0      0.0      0.0  \n",
      "\n",
      "[5 rows x 10188 columns]\n",
      "\n",
      "ðŸ”¹ Sklearn TF-IDF (First 5 rows):\n",
      "   000   07   10  100  1000  100kph  100ks  100th  101  101kph  ...  zipping  \\\n",
      "0  0.0  0.0  0.0  0.0   0.0     0.0    0.0    0.0  0.0     0.0  ...      0.0   \n",
      "1  0.0  0.0  0.0  0.0   0.0     0.0    0.0    0.0  0.0     0.0  ...      0.0   \n",
      "2  0.0  0.0  0.0  0.0   0.0     0.0    0.0    0.0  0.0     0.0  ...      0.0   \n",
      "3  0.0  0.0  0.0  0.0   0.0     0.0    0.0    0.0  0.0     0.0  ...      0.0   \n",
      "4  0.0  0.0  0.0  0.0   0.0     0.0    0.0    0.0  0.0     0.0  ...      0.0   \n",
      "\n",
      "   zips  zone  zones  zoning  zoomed  zoomer  zooming  zooms  zoots  \n",
      "0   0.0   0.0    0.0     0.0     0.0     0.0      0.0    0.0    0.0  \n",
      "1   0.0   0.0    0.0     0.0     0.0     0.0      0.0    0.0    0.0  \n",
      "2   0.0   0.0    0.0     0.0     0.0     0.0      0.0    0.0    0.0  \n",
      "3   0.0   0.0    0.0     0.0     0.0     0.0      0.0    0.0    0.0  \n",
      "4   0.0   0.0    0.0     0.0     0.0     0.0      0.0    0.0    0.0  \n",
      "\n",
      "[5 rows x 9412 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"IPL_Match_Highlights_Commentary.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Extract Commentary column\n",
    "df = df[['Commentary']].dropna()\n",
    "\n",
    "# Preprocess text function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
    "    words = text.split()  # Tokenization\n",
    "    words = [word for word in words if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    return words\n",
    "\n",
    "# Apply preprocessing\n",
    "df['Cleaned_Commentary'] = df['Commentary'].apply(preprocess_text)\n",
    "\n",
    "# Get unique words\n",
    "all_words = set(word for words in df['Cleaned_Commentary'] for word in words)\n",
    "\n",
    "# Compute Term Frequency (TF)\n",
    "def compute_tf(text):\n",
    "    word_counts = Counter(text)\n",
    "    total_words = len(text)\n",
    "    return {word: word_counts[word] / total_words for word in word_counts}\n",
    "\n",
    "df['TF'] = df['Cleaned_Commentary'].apply(compute_tf)\n",
    "\n",
    "# Compute Document Frequency (DF)\n",
    "df_list = df['Cleaned_Commentary'].tolist()\n",
    "doc_count = len(df_list)\n",
    "df_counts = {word: sum(1 for text in df_list if word in text) for word in all_words}\n",
    "\n",
    "# Compute Inverse Document Frequency (IDF)\n",
    "idf_values = {word: np.log(doc_count / (df_counts[word] + 1)) for word in all_words}\n",
    "\n",
    "# Compute TF-IDF\n",
    "def compute_tfidf(tf_dict):\n",
    "    return {word: tf_dict[word] * idf_values[word] for word in tf_dict}\n",
    "\n",
    "df['TF-IDF'] = df['TF'].apply(compute_tfidf)\n",
    "\n",
    "# Convert TF-IDF dictionary to DataFrame\n",
    "tfidf_df = pd.DataFrame(df['TF-IDF'].tolist()).fillna(0)\n",
    "\n",
    "# -------------------- Sklearn TF-IDF for Comparison --------------------\n",
    "vectorizer = TfidfVectorizer()\n",
    "sklearn_tfidf = vectorizer.fit_transform(df['Commentary'].astype(str))\n",
    "sklearn_df = pd.DataFrame(sklearn_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display first 5 rows of both TF-IDF implementations\n",
    "print(\"\\nðŸ”¹ TF-IDF from Scratch (First 5 rows):\")\n",
    "print(tfidf_df.head())\n",
    "\n",
    "print(\"\\nðŸ”¹ Sklearn TF-IDF (First 5 rows):\")\n",
    "print(sklearn_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16733398-4c3f-401a-965d-c89c34d2d5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: This is an example sentence to test stopwords removal.\n",
      "Filtered: example sentence test stopwords removal.\n"
     ]
    }
   ],
   "source": [
    "# Manually defining a list of English stopwords\n",
    "custom_stopwords = [\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\n",
    "    \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
    "    \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\",\n",
    "    \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n",
    "    \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\",\n",
    "    \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\",\n",
    "    \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\",\n",
    "    \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\"\n",
    "]\n",
    "\n",
    "# Example usage to remove stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in custom_stopwords]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Test example\n",
    "text = \"This is an example sentence to test stopwords removal.\"\n",
    "filtered_text = remove_stopwords(text)\n",
    "print(\"Original:\", text)\n",
    "print(\"Filtered:\", filtered_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c43e77dc-6c52-4b7e-8b66-4fa4da80a258",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\daksh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\daksh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\daksh/nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\daksh\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m words\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Apply preprocessing to each document\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m preprocessed_docs \u001b[38;5;241m=\u001b[39m [preprocess(doc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Create vocabulary\u001b[39;00m\n\u001b[0;32m     37\u001b[0m vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(word \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m preprocessed_docs \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m doc)\n",
      "Cell \u001b[1;32mIn[13], line 29\u001b[0m, in \u001b[0;36mpreprocess\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     27\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()  \u001b[38;5;66;03m# Lowercasing\u001b[39;00m\n\u001b[0;32m     28\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstring\u001b[38;5;241m.\u001b[39mpunctuation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, text)  \u001b[38;5;66;03m# Remove punctuation\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m words \u001b[38;5;241m=\u001b[39m word_tokenize(text)  \u001b[38;5;66;03m# Tokenize words\u001b[39;00m\n\u001b[0;32m     30\u001b[0m words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)]  \u001b[38;5;66;03m# Remove stopwords\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m words\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\daksh/nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\daksh\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download stopwords if not available\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"IPL_Match_Highlights_Commentary.csv\"  # Update the path if needed\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Select only the 'Commentary' column and drop NaN values\n",
    "df = df[['Commentary']].dropna()\n",
    "\n",
    "# Convert to a list of comments\n",
    "documents = df['Commentary'].tolist()\n",
    "\n",
    "# Preprocessing Function\n",
    "def preprocess(text):\n",
    "    text = text.lower()  # Lowercasing\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)  # Remove punctuation\n",
    "    words = word_tokenize(text)  # Tokenize words\n",
    "    words = [word for word in words if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    return words\n",
    "\n",
    "# Apply preprocessing to each document\n",
    "preprocessed_docs = [preprocess(doc) for doc in documents]\n",
    "\n",
    "# Create vocabulary\n",
    "vocabulary = set(word for doc in preprocessed_docs for word in doc)\n",
    "\n",
    "# Compute Term Frequency (TF)\n",
    "def compute_tf(doc):\n",
    "    word_counts = Counter(doc)\n",
    "    total_words = len(doc)\n",
    "    return {word: word_counts[word] / total_words for word in word_counts}\n",
    "\n",
    "tf_values = [compute_tf(doc) for doc in preprocessed_docs]\n",
    "\n",
    "# Compute Inverse Document Frequency (IDF)\n",
    "def compute_idf(docs, vocab):\n",
    "    num_docs = len(docs)\n",
    "    idf_values = {}\n",
    "    for word in vocab:\n",
    "        containing_docs = sum(1 for doc in docs if word in doc)\n",
    "        idf_values[word] = math.log(num_docs / (1 + containing_docs))  # Using log smoothing\n",
    "    return idf_values\n",
    "\n",
    "idf_values = compute_idf(preprocessed_docs, vocabulary)\n",
    "\n",
    "# Compute TF-IDF for each document\n",
    "tfidf_values = [{word: tf[word] * idf_values[word] for word in tf} for tf in tf_values]\n",
    "\n",
    "# Convert TF-IDF to DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_values).fillna(0)\n",
    "print(\"TF-IDF from Scratch:\")\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Save TF-IDF results to CSV\n",
    "tfidf_df.to_csv(\"tfidf_from_scratch.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cada9ef-9e1d-4273-a7aa-06e7cb492154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF using Scikit-learn:\n",
      "   000   07   10  100  1000  100kph  100ks  100th  101  101kph  ...  zipping  \\\n",
      "0  0.0  0.0  0.0  0.0   0.0     0.0    0.0    0.0  0.0     0.0  ...      0.0   \n",
      "1  0.0  0.0  0.0  0.0   0.0     0.0    0.0    0.0  0.0     0.0  ...      0.0   \n",
      "2  0.0  0.0  0.0  0.0   0.0     0.0    0.0    0.0  0.0     0.0  ...      0.0   \n",
      "3  0.0  0.0  0.0  0.0   0.0     0.0    0.0    0.0  0.0     0.0  ...      0.0   \n",
      "4  0.0  0.0  0.0  0.0   0.0     0.0    0.0    0.0  0.0     0.0  ...      0.0   \n",
      "\n",
      "   zips  zone  zones  zoning  zoomed  zoomer  zooming  zooms  zoots  \n",
      "0   0.0   0.0    0.0     0.0     0.0     0.0      0.0    0.0    0.0  \n",
      "1   0.0   0.0    0.0     0.0     0.0     0.0      0.0    0.0    0.0  \n",
      "2   0.0   0.0    0.0     0.0     0.0     0.0      0.0    0.0    0.0  \n",
      "3   0.0   0.0    0.0     0.0     0.0     0.0      0.0    0.0    0.0  \n",
      "4   0.0   0.0    0.0     0.0     0.0     0.0      0.0    0.0    0.0  \n",
      "\n",
      "[5 rows x 9147 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert raw text documents to TF-IDF matrix using Scikit-learn\n",
    "vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n",
    "tfidf_matrix = vectorizer.fit_transform(df['Commentary'].dropna())\n",
    "\n",
    "# Convert to DataFrame\n",
    "tfidf_sklearn_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"TF-IDF using Scikit-learn:\")\n",
    "print(tfidf_sklearn_df.head())\n",
    "\n",
    "# Save TF-IDF results from Scikit-learn to CSV\n",
    "tfidf_sklearn_df.to_csv(\"tfidf_sklearn.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02f2cef2-4e3a-4b08-b53d-d0f88ce0b16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\daksh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bd26253-9166-42f4-8b8e-4ef15e30e3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\daksh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03c6fb98-ee33-469f-8ff0-1ece2f96fd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Match_id', 'Team', 'Over_num', 'Commentary', 'batsman', 'score'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('IPL_Match_Highlights_Commentary.csv')\n",
    "\n",
    "# Display column names\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4055e910-a2b0-4a39-896d-26eb5dce5b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in dataset: Index(['Match_id', 'Team', 'Over_num', 'Commentary', 'batsman', 'score'], dtype='object')\n",
      "TF-IDF from scratch (first document): {'rcb': 1.0673291740135906, '1st': 0.3333333333333333, 'inns': 0.3333333333333333}\n",
      "\n",
      "TF-IDF using Scikit-learn (first document):\n",
      " 1st     0.285683\n",
      "csk     0.000000\n",
      "dc      0.000000\n",
      "gl      0.000000\n",
      "inns    0.285683\n",
      "kkr     0.000000\n",
      "kxip    0.000000\n",
      "mi      0.000000\n",
      "rcb     0.914752\n",
      "rps     0.000000\n",
      "rr      0.000000\n",
      "srh     0.000000\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('IPL_Match_Highlights_Commentary.csv')\n",
    "\n",
    "# Display column names to identify the correct column\n",
    "print(\"Columns in dataset:\", df.columns)\n",
    "\n",
    "# Identify the correct text column\n",
    "txt_column = None\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':  # Assuming commentary is text-based\n",
    "        txt_column = col\n",
    "        break\n",
    "\n",
    "if txt_column is None:\n",
    "    raise ValueError(\"No suitable text column found in dataset\")\n",
    "\n",
    "# Extract text data\n",
    "documents = df[txt_column].dropna().tolist()\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Compute TF (Term Frequency)\n",
    "def compute_tf(doc):\n",
    "    word_counts = Counter(tokenize(doc))\n",
    "    total_words = len(tokenize(doc))\n",
    "    return {word: count / total_words for word, count in word_counts.items()}\n",
    "\n",
    "# Compute IDF (Inverse Document Frequency)\n",
    "def compute_idf(docs):\n",
    "    N = len(docs)\n",
    "    idf_values = {}\n",
    "    all_words = set(word for doc in docs for word in tokenize(doc))\n",
    "    \n",
    "    for word in all_words:\n",
    "        df_t = sum(1 for doc in docs if word in tokenize(doc))\n",
    "        idf_values[word] = math.log((N + 1) / (df_t + 1)) + 1  # Smoothed IDF\n",
    "    \n",
    "    return idf_values\n",
    "\n",
    "# Compute TF-IDF\n",
    "def compute_tfidf(docs):\n",
    "    idf_values = compute_idf(docs)\n",
    "    tfidf_docs = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        tf = compute_tf(doc)\n",
    "        tfidf = {word: tf[word] * idf_values[word] for word in tf}\n",
    "        tfidf_docs.append(tfidf)\n",
    "    \n",
    "    return tfidf_docs\n",
    "\n",
    "# Compute TF-IDF manually\n",
    "tfidf_manual = compute_tfidf(documents)\n",
    "\n",
    "# Compare with Scikit-learn's TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_sklearn = vectorizer.fit_transform(documents)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert to DataFrame for easy comparison\n",
    "tfidf_sklearn_df = pd.DataFrame(tfidf_sklearn.toarray(), columns=feature_names)\n",
    "\n",
    "# Display results\n",
    "print(\"TF-IDF from scratch (first document):\", tfidf_manual[0])\n",
    "print(\"\\nTF-IDF using Scikit-learn (first document):\\n\", tfidf_sklearn_df.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee9ba77a-0070-40a7-bf5c-b6d345f82f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Identifying the text column...\n",
      "Columns in dataset: Index(['Match_id', 'Team', 'Over_num', 'Commentary', 'batsman', 'score'], dtype='object')\n",
      "Using 'Team' as the text column.\n",
      "Computing TF-IDF manually...\n",
      "Computing TF-IDF using Scikit-learn...\n",
      "TF-IDF from scratch (first document): {'rcb': 1.0673291740135906, '1st': 0.3333333333333333, 'inns': 0.3333333333333333}\n",
      "\n",
      "TF-IDF using Scikit-learn (first document):\n",
      " 1st     0.285683\n",
      "csk     0.000000\n",
      "dc      0.000000\n",
      "gl      0.000000\n",
      "inns    0.285683\n",
      "kkr     0.000000\n",
      "kxip    0.000000\n",
      "mi      0.000000\n",
      "rcb     0.914752\n",
      "rps     0.000000\n",
      "rr      0.000000\n",
      "srh     0.000000\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('IPL_Match_Highlights_Commentary.csv')\n",
    "print(\"Identifying the text column...\")\n",
    "print(\"Columns in dataset:\", df.columns)\n",
    "txt_column = next((col for col in df.columns if df[col].dtype == 'object'), None)\n",
    "if txt_column is None:\n",
    "    raise ValueError(\"No suitable text column found in dataset\")\n",
    "print(f\"Using '{txt_column}' as the text column.\")\n",
    "documents = df[txt_column].dropna().tolist()\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "def compute_tf(doc):\n",
    "    words = tokenize(doc)\n",
    "    word_counts = Counter(words)\n",
    "    total_words = len(words)\n",
    "    return {word: count / total_words for word, count in word_counts.items()}\n",
    "def compute_idf(docs):\n",
    "    N = len(docs)\n",
    "    idf_values = {}\n",
    "    unique_words = set(word for doc in docs for word in tokenize(doc))\n",
    "    \n",
    "    for word in unique_words:\n",
    "        df_t = sum(1 for doc in docs if word in tokenize(doc))\n",
    "        idf_values[word] = math.log((N + 1) / (df_t + 1)) + 1  # Smoothed IDF\n",
    "    \n",
    "    return idf_values\n",
    "def compute_tfidf(docs):\n",
    "    print(\"Computing TF-IDF manually...\")\n",
    "    idf_values = compute_idf(docs)\n",
    "    tfidf_docs = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        tf = compute_tf(doc)\n",
    "        tfidf = {word: tf[word] * idf_values[word] for word in tf}\n",
    "        tfidf_docs.append(tfidf)\n",
    "    \n",
    "    return tfidf_docs\n",
    "tfidf_manual = compute_tfidf(documents)\n",
    "print(\"Computing TF-IDF using Scikit-learn...\")\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_sklearn = vectorizer.fit_transform(documents)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_sklearn_df = pd.DataFrame(tfidf_sklearn.toarray(), columns=feature_names)\n",
    "print(\"TF-IDF from scratch (first document):\", tfidf_manual[0])\n",
    "print(\"\\nTF-IDF using Scikit-learn (first document):\\n\", tfidf_sklearn_df.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d7a019-2104-4d64-a48e-4010ef820f32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
