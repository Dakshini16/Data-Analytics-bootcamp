{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c36634f1-2b41-46dd-aefa-82f6b3a00007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping complete! Data saved to IPL_2022_Commentary.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def get_match_urls(series_url):\n",
    "    \"\"\"Scrape match URLs for the given series.\"\"\"\n",
    "    response = requests.get(series_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    match_links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        if '/match/' in link['href']:  # Adjust based on the actual URL pattern\n",
    "            match_links.append(\"https://www.espncricinfo.com\" + link['href'])\n",
    "    return list(set(match_links))  # Remove duplicates\n",
    "\n",
    "def scrape_match_data(match_url):\n",
    "    \"\"\"Extract ball-by-ball commentary and match details.\"\"\"\n",
    "    response = requests.get(match_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract metadata\n",
    "    match_name = soup.find('h1').text if soup.find('h1') else \"Unknown\"\n",
    "    \n",
    "    details = soup.find_all(\"span\", class_=\"ds-text-tight-m\")\n",
    "    match_date, match_venue = \"\", \"\"\n",
    "    if details and len(details) >= 2:\n",
    "        match_date = details[0].text.strip()\n",
    "        match_venue = details[1].text.strip()\n",
    "    \n",
    "    # Extract scores\n",
    "    teams = soup.find_all(\"span\", class_=\"ds-text-title\")\n",
    "    scores = soup.find_all(\"strong\")\n",
    "    team1, team2, score1, score2 = \"\", \"\", \"\", \"\"\n",
    "    if teams and scores and len(teams) >= 2 and len(scores) >= 2:\n",
    "        team1, team2 = teams[0].text.strip(), teams[1].text.strip()\n",
    "        score1, score2 = scores[0].text.strip(), scores[1].text.strip()\n",
    "    \n",
    "    # Extract match result\n",
    "    result = \"\"\n",
    "    result_tag = soup.find(\"p\", class_=\"ds-text-tight-m\")\n",
    "    if result_tag:\n",
    "        result = result_tag.text.strip()\n",
    "    \n",
    "    # Extract ball-by-ball commentary\n",
    "    commentary_data = []\n",
    "    for comment in soup.find_all('div', class_='ds-text-tight-m'):  # Adjust class based on site structure\n",
    "        ball_info = comment.text.strip()\n",
    "        if ball_info:\n",
    "            commentary_data.append([match_name, match_date, match_venue, team1, team2, score1, score2, result, ball_info])\n",
    "    \n",
    "    return commentary_data\n",
    "\n",
    "def main():\n",
    "    series_url = \"https://www.espncricinfo.com/series/ipl-2022-1298423\"  # Adjust based on actual series URL\n",
    "    match_urls = get_match_urls(series_url)\n",
    "    all_data = []\n",
    "    \n",
    "    for match_url in match_urls:\n",
    "        print(f\"Scraping: {match_url}\")\n",
    "        match_data = scrape_match_data(match_url)\n",
    "        all_data.extend(match_data)\n",
    "        time.sleep(2)  # Respectful scraping\n",
    "    \n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame(all_data, columns=[\"Match Name\", \"Match Date\", \"Match Venue\", \"Team 1\", \"Team 2\", \"Team 1 Score\", \"Team 2 Score\", \"Match Won By\", \"Ball Commentary\"])\n",
    "    df.to_csv(\"IPL_2022_Commentary.csv\", index=False)\n",
    "    print(\"Scraping complete! Data saved to IPL_2022_Commentary.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff3abbad-4ed7-4d1f-8176-39c877a784b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping complete! Data saved to IPL_2022_Commentary.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def get_match_urls(series_url):\n",
    "    \"\"\"Scrape match URLs for the given series.\"\"\"\n",
    "    response = requests.get(series_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    match_links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        if '/full-scorecard' in link['href']:  # Ensuring we get full match details\n",
    "            match_links.append(\"https://www.espncricinfo.com\" + link['href'])\n",
    "    return list(set(match_links))  # Remove duplicates\n",
    "\n",
    "def scrape_match_data(match_url):\n",
    "    \"\"\"Extract ball-by-ball commentary and match details.\"\"\"\n",
    "    response = requests.get(match_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract metadata\n",
    "    match_name = soup.find('h1').text if soup.find('h1') else \"Unknown\"\n",
    "    \n",
    "    details = soup.find_all(\"div\", class_=\"ds-text-tight-m ds-font-regular\")\n",
    "    match_date, match_venue = \"\", \"\"\n",
    "    if details and len(details) >= 2:\n",
    "        match_date = details[0].text.strip()\n",
    "        match_venue = details[1].text.strip()\n",
    "    \n",
    "    # Extract scores\n",
    "    teams = soup.find_all(\"span\", class_=\"ds-text-title\")\n",
    "    scores = soup.find_all(\"div\", class_=\"ds-text-compact-m\")\n",
    "    team1, team2, score1, score2 = \"\", \"\", \"\", \"\"\n",
    "    if teams and len(teams) >= 2:\n",
    "        team1, team2 = teams[0].text.strip(), teams[1].text.strip()\n",
    "    if scores and len(scores) >= 2:\n",
    "        score1, score2 = scores[0].text.strip(), scores[1].text.strip()\n",
    "    \n",
    "    # Extract match result\n",
    "    result = \"\"\n",
    "    result_tag = soup.find(\"p\", class_=\"ds-text-tight-m\")\n",
    "    if result_tag:\n",
    "        result = result_tag.text.strip()\n",
    "    \n",
    "    # Extract ball-by-ball commentary\n",
    "    commentary_data = []\n",
    "    for comment in soup.find_all('div', class_='ci-html-content'):  # Adjust class based on site structure\n",
    "        ball_info = comment.text.strip()\n",
    "        if ball_info:\n",
    "            commentary_data.append([match_name, match_date, match_venue, team1, team2, score1, score2, result, ball_info])\n",
    "    \n",
    "    return commentary_data\n",
    "\n",
    "def main():\n",
    "    series_url = \"https://www.espncricinfo.com/series/ipl-2022-1298423\"  # Adjust based on actual series URL\n",
    "    match_urls = get_match_urls(series_url)\n",
    "    all_data = []\n",
    "    \n",
    "    for match_url in match_urls:\n",
    "        print(f\"Scraping: {match_url}\")\n",
    "        match_data = scrape_match_data(match_url)\n",
    "        all_data.extend(match_data)\n",
    "        time.sleep(2)  # Respectful scraping\n",
    "    \n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame(all_data, columns=[\"Match Name\", \"Match Date\", \"Match Venue\", \"Team 1\", \"Team 2\", \"Team 1 Score\", \"Team 2 Score\", \"Match Won By\", \"Ball Commentary\"])\n",
    "    df.to_csv(\"IPL_2022_Commentary.csv\", index=False)\n",
    "    print(\"Scraping complete! Data saved to IPL_2022_Commentary.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "429786d2-08a2-4de7-a765-cfe1d181caa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\daksh\\appdata\\roaming\\python\\python312\\site-packages (4.29.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\daksh\\appdata\\roaming\\python\\python312\\site-packages (2.2.3)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\daksh\\appdata\\roaming\\python\\python312\\site-packages (4.0.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\daksh\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\daksh\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (0.12.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\programdata\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from webdriver-manager) (24.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\daksh\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.17->selenium) (25.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\daksh\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\daksh\\appdata\\roaming\\python\\python312\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.3.2)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium pandas webdriver-manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed4dd04f-4ece-421b-b332-3bf81b0eecc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraping Complete! Data saved to Cricbuzz_Ball_by_Ball_Commentary.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def get_commentary(url):\n",
    "    \"\"\"Scrape ball-by-ball commentary from Cricbuzz using Selenium.\"\"\"\n",
    "\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Run without opening browser\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # Allow JavaScript to load\n",
    "\n",
    "    match_title = driver.title.split('|')[0].strip()\n",
    "\n",
    "    commentary_data = []\n",
    "    balls = driver.find_elements(\"xpath\", \"//div[contains(@class, 'cb-col cb-col-100 cb-com-ln')]\")\n",
    "    \n",
    "    for ball in balls:\n",
    "        commentary_text = ball.text.strip()\n",
    "        if commentary_text:\n",
    "            commentary_data.append([match_title, commentary_text])\n",
    "\n",
    "    driver.quit()\n",
    "    return commentary_data\n",
    "\n",
    "def main():\n",
    "    match_url = \"https://www.cricbuzz.com/live-cricket-scores/40381\"  # Replace with any live match\n",
    "    data = get_commentary(match_url)\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[\"Match Name\", \"Ball Commentary\"])\n",
    "    df.to_csv(\"Cricbuzz_Ball_by_Ball_Commentary.csv\", index=False)\n",
    "    print(\"✅ Scraping Complete! Data saved to Cricbuzz_Ball_by_Ball_Commentary.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9aa4577-96d0-46be-822a-84edd4f94f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully fetched page!\n",
      "❌ Commentary section not found. Cricbuzz may have updated its structure.\n",
      "\n",
      "❌ No data extracted. Cricbuzz may have changed its page structure. Try changing selectors.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Cricbuzz commentary page URL (change the match ID if needed)\n",
    "url = \"https://www.cricbuzz.com/live-cricket-scores/66168/gt-vs-rr-final-indian-premier-league-2023\"\n",
    "\n",
    "# Headers to avoid bot detection\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Fetch page content\n",
    "response = requests.get(url, headers=headers)\n",
    "if response.status_code != 200:\n",
    "    print(f\"❌ Failed to fetch page. Status code: {response.status_code}\")\n",
    "else:\n",
    "    print(\"✅ Successfully fetched page!\")\n",
    "\n",
    "# Parse the page content\n",
    "soup = BeautifulSoup(response.content, \"lxml\")\n",
    "\n",
    "# Find ball-by-ball commentary section\n",
    "commentary_section = soup.find(\"div\", class_=\"cb-col cb-col-100 cb-com-ln\")\n",
    "\n",
    "if not commentary_section:\n",
    "    print(\"❌ Commentary section not found. Cricbuzz may have updated its structure.\")\n",
    "else:\n",
    "    print(\"✅ Commentary section found!\")\n",
    "\n",
    "# Extract all commentary events\n",
    "commentary_entries = soup.find_all(\"div\", class_=\"cb-col cb-col-100 cb-col-rt cb-com-ln\")\n",
    "\n",
    "# Initialize list to store data\n",
    "data = []\n",
    "\n",
    "# Loop through each commentary entry\n",
    "for entry in commentary_entries:\n",
    "    text = entry.text.strip()\n",
    "\n",
    "    if not text:\n",
    "        continue  # Skip empty entries\n",
    "\n",
    "    # Example: \"19.2 Boult to Gill, FOUR!\"\n",
    "    parts = text.split(\" \")\n",
    "    if len(parts) < 3:\n",
    "        continue\n",
    "\n",
    "    # Extract Over and Ball Number\n",
    "    over_ball = parts[0]\n",
    "    if \".\" not in over_ball:\n",
    "        continue\n",
    "    over, ball_no = map(int, over_ball.split(\".\"))\n",
    "\n",
    "    # Extract Bowler and Batter Name\n",
    "    bowler_name = parts[1]\n",
    "    batter_name = parts[3]\n",
    "\n",
    "    # Extract Ball Type and Runs\n",
    "    if \"FOUR\" in text:\n",
    "        ball_type = \"legal\"\n",
    "        shot_type = \"boundary\"\n",
    "        runs = 4\n",
    "    elif \"SIX\" in text:\n",
    "        ball_type = \"legal\"\n",
    "        shot_type = \"boundary\"\n",
    "        runs = 6\n",
    "    elif \"wide\" in text:\n",
    "        ball_type = \"wide\"\n",
    "        shot_type = \"none\"\n",
    "        runs = 1\n",
    "    elif \"no ball\" in text:\n",
    "        ball_type = \"no ball\"\n",
    "        shot_type = \"none\"\n",
    "        runs = 1\n",
    "    else:\n",
    "        ball_type = \"legal\"\n",
    "        shot_type = \"other\"\n",
    "        runs = 0\n",
    "\n",
    "    # Append to list\n",
    "    data.append([ball_no, over, bowler_name, batter_name, ball_type, shot_type, runs])\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Ball No\", \"Over\", \"Bowler Name\", \"Batter Name\", \"Ball Type\", \"Shot Type\", \"Runs Scored\"])\n",
    "\n",
    "# Display DataFrame\n",
    "if not df.empty:\n",
    "    print(\"\\n✅ Extracted Ball-by-Ball Commentary Data:\\n\")\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"\\n❌ No data extracted. Cricbuzz may have changed its page structure. Try changing selectors.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf4b8a32-70ab-4101-b87a-0aea0c0791b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ball No</th>\n",
       "      <th>Over</th>\n",
       "      <th>Bowler</th>\n",
       "      <th>Batter</th>\n",
       "      <th>Ball Type</th>\n",
       "      <th>Shot Type</th>\n",
       "      <th>Runs Scored</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>Boult</td>\n",
       "      <td>Gill</td>\n",
       "      <td>legal</td>\n",
       "      <td>boundary</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>Boult</td>\n",
       "      <td>Gill</td>\n",
       "      <td>legal</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>Chahal</td>\n",
       "      <td>Miller</td>\n",
       "      <td>legal</td>\n",
       "      <td>wicket</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>Chahal</td>\n",
       "      <td>Miller</td>\n",
       "      <td>legal</td>\n",
       "      <td>boundary</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ball No  Over  Bowler  Batter Ball Type Shot Type  Runs Scored\n",
       "0        2    19   Boult    Gill     legal  boundary            4\n",
       "1        1    19   Boult    Gill     legal     other            1\n",
       "2        6    18  Chahal  Miller     legal    wicket            0\n",
       "3        5    18  Chahal  Miller     legal  boundary            6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 🏏 Paste Cricbuzz commentary here (Manually copied from Cricbuzz)\n",
    "commentary_text = \"\"\"\n",
    "19.2 Boult to Gill, FOUR! Crunched past cover\n",
    "19.1 Boult to Gill, 1 run, taps to point, quick single\n",
    "18.6 Chahal to Miller, OUT! Caught at deep mid-wicket\n",
    "18.5 Chahal to Miller, SIX! Massive hit over long-on\n",
    "\"\"\"\n",
    "\n",
    "# 🏏 Process commentary line by line\n",
    "data = []\n",
    "for line in commentary_text.strip().split(\"\\n\"):\n",
    "    match = re.match(r\"(\\d+)\\.(\\d+) (\\w+) to (\\w+), (.*)\", line)\n",
    "    \n",
    "    if match:\n",
    "        over = int(match.group(1))\n",
    "        ball_no = int(match.group(2))\n",
    "        bowler = match.group(3)\n",
    "        batter = match.group(4)\n",
    "        ball_details = match.group(5)\n",
    "\n",
    "        # Determine ball type & shot type\n",
    "        ball_type = \"legal\"\n",
    "        shot_type = \"other\"\n",
    "        runs = 0\n",
    "\n",
    "        if \"FOUR\" in ball_details:\n",
    "            shot_type = \"boundary\"\n",
    "            runs = 4\n",
    "        elif \"SIX\" in ball_details:\n",
    "            shot_type = \"boundary\"\n",
    "            runs = 6\n",
    "        elif \"OUT\" in ball_details:\n",
    "            shot_type = \"wicket\"\n",
    "        elif \"1 run\" in ball_details:\n",
    "            runs = 1\n",
    "        elif \"2 runs\" in ball_details:\n",
    "            runs = 2\n",
    "        elif \"3 runs\" in ball_details:\n",
    "            runs = 3\n",
    "        elif \"wide\" in ball_details:\n",
    "            ball_type = \"wide\"\n",
    "            runs = 1\n",
    "        elif \"no ball\" in ball_details:\n",
    "            ball_type = \"no ball\"\n",
    "            runs = 1\n",
    "\n",
    "        data.append([ball_no, over, bowler, batter, ball_type, shot_type, runs])\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Ball No\", \"Over\", \"Bowler\", \"Batter\", \"Ball Type\", \"Shot Type\", \"Runs Scored\"])\n",
    "\n",
    "# 📊 Display Data in Jupyter Notebook\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30ea82d-4a06-414a-bb30-16c5ecf94a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
